{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chsrinidh-rao/Ai-Assistant-coding/blob/main/AI_LAB_5_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eHHJlaHs-zu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "776a287e"
      },
      "source": [
        "### Data Collection Script: Safeguarding User Information\n",
        "\n",
        "This script demonstrates how to collect basic user data safely, display it, and crucially, outlines essential considerations for data anonymization, protection, and ethical handling. Protecting personal data is paramount in any application."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "187f035b",
        "outputId": "5be28e64-b73b-4cfd-d6f9-ad89529f4b05"
      },
      "source": [
        "# Python script to collect basic user data\n",
        "\n",
        "import re # Required for email validation\n",
        "import hashlib # Required for hashing identifiers\n",
        "\n",
        "def collect_user_data():\n",
        "    \"\"\"Collects user's name, age, and email address safely.\"\"\"\n",
        "    print(\"--- User Data Collection ---\")\n",
        "\n",
        "    # 1. Safely collect user input\n",
        "    # Name: Basic string input\n",
        "    name = input(\"Please enter your name: \").strip()\n",
        "    while not name: # Basic validation: ensure name is not empty\n",
        "        print(\"Name cannot be empty. Please try again.\")\n",
        "        name = input(\"Please enter your name: \").strip()\n",
        "\n",
        "    # Age: Integer input with validation\n",
        "    age = None\n",
        "    while age is None:\n",
        "        try:\n",
        "            age_str = input(\"Please enter your age: \").strip()\n",
        "            if not age_str.isdigit(): # Check if input is purely digits\n",
        "                raise ValueError(\"Age must be a number.\")\n",
        "            age = int(age_str)\n",
        "            if age < 0 or age > 120: # Reasonable age range validation\n",
        "                raise ValueError(\"Age must be between 0 and 120.\")\n",
        "        except ValueError as e:\n",
        "            print(f\"Invalid input for age: {e}. Please enter a valid number.\")\n",
        "            age = None\n",
        "\n",
        "    # Email: String input with basic regex validation\n",
        "    email = None\n",
        "    email_regex = r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,4}$\"\n",
        "    while email is None:\n",
        "        email_input = input(\"Please enter your email address: \").strip().lower()\n",
        "        if re.match(email_regex, email_input):\n",
        "            email = email_input\n",
        "        else:\n",
        "            print(\"Invalid email format. Please enter a valid email address.\")\n",
        "\n",
        "    user_data = {\n",
        "        \"name\": name,\n",
        "        \"age\": age,\n",
        "        \"email\": email\n",
        "    }\n",
        "    return user_data\n",
        "\n",
        "def display_user_data(data):\n",
        "    \"\"\"Displays the collected user data clearly.\"\"\"\n",
        "    print(\"\\n--- Collected User Data ---\")\n",
        "    print(f\"Name: {data['name']}\")\n",
        "    print(f\"Age: {data['age']}\")\n",
        "    print(f\"Email: {data['email']}\")\n",
        "\n",
        "    # Example of anonymization for display or storage (masking)\n",
        "    masked_email = data['email'][0] + \"*\" * (data['email'].find('@') - 2) + data['email'][data['email'].find('@')-1:] if '@' in data['email'] else \"*****\"\n",
        "    print(f\"Masked Email (example anonymization): {masked_email}\")\n",
        "\n",
        "    # Example of hashing for display or storage\n",
        "    # Hashing converts data into a fixed-size string of characters.\n",
        "    # It's a one-way process, making it suitable for checking data integrity or\n",
        "    # storing identifiers that don't need to be decrypted.\n",
        "    # SHA256 is a common cryptographic hash function.\n",
        "    name_hash = hashlib.sha256(data['name'].encode()).hexdigest()\n",
        "    email_hash = hashlib.sha256(data['email'].encode()).hexdigest()\n",
        "    print(f\"Name Hash (SHA256): {name_hash}\")\n",
        "    print(f\"Email Hash (SHA256): {email_hash}\")\n",
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    user_info = collect_user_data()\n",
        "    display_user_data(user_info)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- User Data Collection ---\n",
            "Please enter your name:  srinidh\n",
            "Please enter your age: 19\n",
            "Please enter your email address: srinidh@gmail.comm\n",
            "\n",
            "--- Collected User Data ---\n",
            "Name: srinidh\n",
            "Age: 19\n",
            "Email: srinidh@gmail.comm\n",
            "Masked Email (example anonymization): s*****h@gmail.comm\n",
            "Name Hash (SHA256): 6b964a14bc503e876dcee2fd2d964b4a38f21b99cf301dd782993a2ae555445c\n",
            "Email Hash (SHA256): 2084d279cc29c04d558b30e5ee8c01c56191e6636a361b60c2eecb6bbc57306a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hMevd8kOz1nS"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b995848",
        "outputId": "b09bec6e-6feb-4b12-c9ce-d05f25b74320"
      },
      "source": [
        "# Python script for training and evaluating a simple machine learning model with Ethical AI considerations\n",
        "\n",
        "import pandas as pd # Suggestion: For data manipulation and structuring\n",
        "import numpy as np # Suggestion: For numerical operations, especially in data generation\n",
        "from sklearn.model_selection import train_test_split # Suggestion: For splitting data into training and testing sets\n",
        "from sklearn.tree import DecisionTreeClassifier # Suggestion: A simple, interpretable model for demonstration\n",
        "from sklearn.metrics import classification_report, accuracy_score # Suggestion: For model evaluation metrics\n",
        "\n",
        "# --- 1. Data Generation with Intentional Bias ---\n",
        "# Suggestion: For academic purposes, generating synthetic data allows for controlled exploration of bias.\n",
        "# In real-world scenarios, data collection itself needs rigorous ethical oversight to prevent inherited biases.\n",
        "\n",
        "n_samples = 1000 # Number of synthetic data samples\n",
        "np.random.seed(42) # For reproducibility of random data generation\n",
        "\n",
        "# Features: age, income, and a 'proxy_feature' that might represent a protected group.\n",
        "age = np.random.randint(18, 70, n_samples)\n",
        "income = np.random.randint(20000, 150000, n_samples)\n",
        "\n",
        "# Ethical Consideration: Introducing a 'proxy_feature'. In real-world data, even if direct sensitive attributes\n",
        "# (like gender or race) are excluded, other features (e.g., zip code, certain behavioral patterns) can act as proxies\n",
        "# and implicitly carry the same biases. Here, we simulate this by creating 'group_A' (0) and 'group_B' (1).\n",
        "proxy_feature = np.random.choice([0, 1], n_samples, p=[0.5, 0.5]) # 0 for 'Group A', 1 for 'Group B'\n",
        "\n",
        "# Target variable: 'approved' (1) or 'denied' (0). Simulating a binary decision, e.g., loan approval.\n",
        "# We create a base approval logic but then *intentionally* introduce bias against 'Group A'.\n",
        "approved = np.zeros(n_samples, dtype=int)\n",
        "\n",
        "for i in range(n_samples):\n",
        "    # Base approval logic: Generally higher approval for higher income and middle age.\n",
        "    if income[i] > 60000 and 30 < age[i] < 60:\n",
        "        approved[i] = 1\n",
        "    elif income[i] > 40000 and 25 < age[i] < 65 and np.random.rand() < 0.7: # Slight chance for others\n",
        "        approved[i] = 1\n",
        "\n",
        "    # Intentional Bias: A higher denial chance for 'Group A' (proxy_feature=0), overriding other factors.\n",
        "    # This mimics real-world scenarios where historical biases or discriminatory practices are embedded in data.\n",
        "    if proxy_feature[i] == 0: # If in 'Group A'\n",
        "        if np.random.rand() < 0.6: # 60% chance to be denied, even if other factors suggest approval\n",
        "            approved[i] = 0\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'age': age,\n",
        "    'income': income,\n",
        "    'proxy_feature': proxy_feature, # This feature will demonstrate bias\n",
        "    'approved': approved\n",
        "})\n",
        "\n",
        "# Display first few rows of the generated data to understand its structure.\n",
        "print(\"--- Sample of Generated Data ---\")\n",
        "print(df.head())\n",
        "\n",
        "# --- 2. Data Splitting ---\n",
        "# Separate features (X) and target (y).\n",
        "X = df[['age', 'income', 'proxy_feature']]\n",
        "y = df['approved']\n",
        "\n",
        "# Split data into training and testing sets.\n",
        "# Ethical Consideration: Data splitting should be done carefully. If bias exists in the overall dataset,\n",
        "# it will be propagated to both training and test sets. Ensure test sets are representative of all subgroups.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "print(f\"\\nTraining set size: {len(X_train)} samples\")\n",
        "print(f\"Testing set size: {len(X_test)} samples\")\n",
        "\n",
        "# --- 3. Model Training ---\n",
        "# Suggestion: Decision Trees are chosen for their relative interpretability, making it easier to explain predictions.\n",
        "# How the model works at a high level (Explainability and Transparency):\n",
        "# A Decision Tree learns by recursively splitting the data into subsets based on feature values.\n",
        "# It seeks to find the feature and split-point (e.g., 'income > $60,000' or 'age < 30') that best separates the classes (e.g., 'approved' vs. 'denied').\n",
        "# This process continues until a stopping criterion is met (e.g., `max_depth`), forming a tree-like structure of decisions.\n",
        "# Each leaf node in the tree represents a final classification decision (e.g., 'approve' or 'deny').\n",
        "# This \"if-then-else\" logic makes Decision Trees quite transparent; you can literally trace the path from input features to output prediction.\n",
        "\n",
        "model = DecisionTreeClassifier(max_depth=5, random_state=42) # Limit depth for a simpler, more explainable tree.\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\n--- Model Training Complete ---\")\n",
        "\n",
        "# --- 4. Prediction ---\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# --- 5. Evaluation ---\n",
        "print(\"\\n--- Model Evaluation ---\")\n",
        "overall_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Overall Accuracy: {overall_accuracy:.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Accuracy Limitations and Why Results May Not Generalize:\n",
        "# This is a simple Decision Tree model trained on a small, synthetic dataset.\n",
        "# 1. Simplicity of features: Real-world problems involve far more complex and nuanced features.\n",
        "# 2. Overfitting: Even with `max_depth` limited, simple models can sometimes overfit to training data, performing poorly on unseen data.\n",
        "# 3. Limited data: Small datasets might not capture the full variance and complexity of real-world scenarios, leading to poor generalization.\n",
        "# Developer Responsibility: Always validate model performance on diverse, real-world data from multiple sources and monitor it continuously post-deployment.\n",
        "\n",
        "# --- 6. Analysis of Bias and Fairness Considerations ---\n",
        "\n",
        "print(\"\\n--- Bias and Fairness Analysis ---\")\n",
        "\n",
        "# Potential Sources of Bias in Training Data or Features:\n",
        "# 1. Training Data Bias (Demonstrated here): Our synthetic data was *intentionally* biased where 'proxy_feature'=0 (Group A)\n",
        "#    had a higher denial rate. In real-world data, such biases can arise from:\n",
        "#    - Historical discrimination reflected in past human decisions that become labels in the data.\n",
        "#    - Unrepresentative sampling during data collection, leading to under-representation of certain groups.\n",
        "#    - Measurement errors that disproportionately affect specific subgroups.\n",
        "# 2. Feature Bias: Even if direct sensitive attributes (e.g., race, gender) are excluded, other features used in the model\n",
        "#    might serve as proxies. For example, a credit score model might use zip code, which could correlate with race or income.\n",
        "# 3. Selection Bias: If the data used to train the model is not randomly sampled from the population it's meant to serve.\n",
        "\n",
        "# Fairness Considerations and Risks of Misuse:\n",
        "# - Disparate Impact: Does the model perform differently or lead to different outcomes for different subgroups?\n",
        "#   Let's check the accuracy for our intentionally biased 'proxy_feature' subgroups.\n",
        "X_test_group0 = X_test[X_test['proxy_feature'] == 0] # Filter for Group A\n",
        "y_test_group0 = y_test[X_test['proxy_feature'] == 0]\n",
        "y_pred_group0 = model.predict(X_test_group0)\n",
        "\n",
        "X_test_group1 = X_test[X_test['proxy_feature'] == 1] # Filter for Group B\n",
        "y_test_group1 = y_test[X_test['proxy_feature'] == 1]\n",
        "y_pred_group1 = model.predict(X_test_group1)\n",
        "\n",
        "accuracy_group0 = accuracy_score(y_test_group0, y_pred_group0)\n",
        "accuracy_group1 = accuracy_score(y_test_group1, y_pred_group1)\n",
        "\n",
        "print(f\"Accuracy for Group A (proxy_feature=0): {accuracy_group0:.4f}\")\n",
        "print(f\"Accuracy for Group B (proxy_feature=1): {accuracy_group1:.4f}\")\n",
        "\n",
        "# As shown above, due to the intentional bias in data generation, the model's accuracy (and other fairness metrics)\n",
        "# can be significantly different between groups. This demonstrates a clear disparate impact, where one group is\n",
        "# systematically disadvantaged by the model's decisions.\n",
        "#\n",
        "# Risks of Misuse:\n",
        "# - Discrimination: Models trained on biased data can perpetuate and even amplify historical societal discrimination\n",
        "#   in critical areas like loan applications, hiring decisions, healthcare, or criminal justice.\n",
        "# - Unfair Resource Allocation: If the model is used to allocate limited resources, biased predictions can lead to\n",
        "#   certain groups being unfairly disadvantaged or underserved.\n",
        "# - Erosion of Trust: Users and the public will lose trust in AI systems that are perceived as unfair, biased, or discriminatory.\n",
        "\n",
        "# Developer Responsibility and Human Oversight When Deploying AI-Generated Models:\n",
        "# - Continuous Monitoring: AI models are not static. Their performance and fairness must be continuously monitored\n",
        "#   post-deployment. Data distributions can shift, and subtle biases can emerge over time (concept drift, data drift).\n",
        "# - Auditing for Bias: Regularly audit models for different types of bias and fairness metrics across various demographic\n",
        "#   or protected groups. Utilize specialized fairness toolkits (e.g., IBM's AI Fairness 360, Google's What-If Tool) to\n",
        "#   identify, quantify, and mitigate bias.\n",
        "# - Human-in-the-Loop: For high-stakes decisions (e.g., medical diagnoses, critical financial approvals, legal outcomes),\n",
        "#   human oversight and intervention are absolutely crucial. The AI should serve as an aid, providing insights and recommendations,\n",
        "#   but not acting as the sole or final decision-maker.\n",
        "# - Transparency with Users: Clearly communicate the model's purpose, capabilities, limitations, and potential biases to end-users.\n",
        "#   Provide mechanisms for appeal or correction if users feel they have been unfairly treated by an AI-driven decision.\n",
        "# - Ethical Deployment: Thoroughly consider the broader societal impact and potential harms before deploying any AI model.\n",
        "#   Ensure the model aligns with ethical principles, legal requirements (e.g., GDPR, anti-discrimination laws), and company values.\n",
        "# - Responsible AI Principle: Embed fairness, accountability, and transparency throughout the entire AI lifecycle,\n",
        "#   from initial problem definition and data collection to model deployment, monitoring, and eventual decommissioning.\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Sample of Generated Data ---\n",
            "   age  income  proxy_feature  approved\n",
            "0   56  125186              1         1\n",
            "1   69   54674              1         0\n",
            "2   46   55854              1         1\n",
            "3   32   66271              1         1\n",
            "4   60   93688              1         0\n",
            "\n",
            "Training set size: 700 samples\n",
            "Testing set size: 300 samples\n",
            "\n",
            "--- Model Training Complete ---\n",
            "\n",
            "--- Model Evaluation ---\n",
            "Overall Accuracy: 0.8467\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.93      0.89       203\n",
            "           1       0.81      0.68      0.74        97\n",
            "\n",
            "    accuracy                           0.85       300\n",
            "   macro avg       0.84      0.80      0.82       300\n",
            "weighted avg       0.84      0.85      0.84       300\n",
            "\n",
            "\n",
            "--- Bias and Fairness Analysis ---\n",
            "Accuracy for Group A (proxy_feature=0): 0.8289\n",
            "Accuracy for Group B (proxy_feature=1): 0.8649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcdb03de",
        "outputId": "15e64994-b4c6-458d-ecb9-2f004bdb59ee"
      },
      "source": [
        "import logging\n",
        "import os\n",
        "import re # Suggestion: For potential redaction/masking of sensitive patterns if they bypass initial filters\n",
        "\n",
        "# --- 1. Configure the Logger ---\n",
        "\n",
        "def setup_logging(log_file='app.log', level=logging.INFO):\n",
        "    \"\"\"Configures the application logger with a file handler and console handler.\"\"\"\n",
        "\n",
        "    # Developer Responsibility: Logging configuration should be part of a secure application setup.\n",
        "    # Ensure logs are stored in a secure location with appropriate access controls.\n",
        "\n",
        "    logger = logging.getLogger(__name__)\n",
        "    logger.setLevel(level)\n",
        "\n",
        "    # Avoid duplicate handlers if setup is called multiple times\n",
        "    if not logger.handlers:\n",
        "        # File Handler: For persistent logs\n",
        "        file_handler = logging.FileHandler(log_file)\n",
        "        file_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "        file_handler.setFormatter(file_formatter)\n",
        "        logger.addHandler(file_handler)\n",
        "\n",
        "        # Console Handler: For real-time monitoring during development/operations\n",
        "        console_handler = logging.StreamHandler()\n",
        "        console_formatter = logging.Formatter('%(name)s - %(levelname)s - %(message)s')\n",
        "        console_handler.setFormatter(console_formatter)\n",
        "        logger.addHandler(console_handler)\n",
        "\n",
        "    logger.info(\"Logging configured.\")\n",
        "    return logger\n",
        "\n",
        "# Initialize the logger\n",
        "logger = setup_logging()\n",
        "\n",
        "\n",
        "# --- 2. Log Common Application Events ---\n",
        "\n",
        "def handle_request(request_data: dict):\n",
        "    \"\"\"Simulates handling an incoming web request and logs relevant information.\n",
        "\n",
        "    Ethical Logging Practice: Log events at appropriate levels (INFO for normal ops, WARNING for unusual, ERROR for failures).\n",
        "    \"\"\"\n",
        "\n",
        "    # Developer Responsibility: When logging request data, *always* sanitize it first.\n",
        "    # Sensitive data MUST NEVER be written to logs in plain text.\n",
        "    # Why sensitive data must not be logged: Logs are often less secure than databases,\n",
        "    # can be accessed by more personnel, and accumulate over time, increasing the risk of exposure\n",
        "    # during breaches, audits, or even accidental disclosures.\n",
        "\n",
        "    # Examples of data that should be masked, redacted, or excluded from logs:\n",
        "    # - Passwords, API Keys, Authentication Tokens (e.g., JWTs, session IDs)\n",
        "    # - Personally Identifiable Information (PII) like full names, email addresses, phone numbers, SSNs, credit card numbers\n",
        "    # - Health Information (PHI), Financial Data\n",
        "    # - Geolocation data with high precision\n",
        "\n",
        "    logged_data = request_data.copy() # Suggestion: Work on a copy to avoid altering original request_data\n",
        "\n",
        "    # Data Exclusion/Redaction Example:\n",
        "    if 'password' in logged_data: # Suggestion: If 'password' exists in request, remove it.\n",
        "        del logged_data['password']\n",
        "    if 'auth_token' in logged_data: # Suggestion: Remove sensitive tokens.\n",
        "        del logged_data['auth_token']\n",
        "\n",
        "    # Data Masking Example:\n",
        "    if 'email' in logged_data: # Suggestion: Mask email addresses before logging.\n",
        "        email = logged_data['email']\n",
        "        # Basic masking: show first char, then ***, then domain\n",
        "        masked_email = email[0] + '***' + email[email.find('@'):] if '@' in email else '***masked***'\n",
        "        logged_data['email'] = masked_email\n",
        "\n",
        "    logger.info(f\"Request received: {logged_data}\")\n",
        "    # Ethical Logging Practice: Log just enough information to debug and audit, but no more.\n",
        "\n",
        "def process_data(user_id: str, data: str):\n",
        "    \"\"\"Simulates a successful data processing action.\"\"\"\n",
        "    # Developer Responsibility: Ensure that any identifiers (like user_id) logged are either essential for auditing\n",
        "    # and stored securely, or are pseudonymized/hashed if possible, especially in high-volume logs.\n",
        "    logger.info(f\"Data successfully processed for user_id: {user_id}. Data summary: {data[:20]}...\")\n",
        "\n",
        "def simulate_error(error_message: str, user_context: dict = None):\n",
        "    \"\"\"Simulates an error condition and logs it.\n",
        "\n",
        "    Ethical Logging Practice: Error logs often contain stack traces which might unintentionally expose sensitive data.\n",
        "    Configure loggers to redact potentially sensitive paths or variables from stack traces if possible, or review logs regularly.\n",
        "    \"\"\"\n",
        "    context = user_context.copy() if user_context else {}\n",
        "    if 'user_email' in context: # Suggestion: Redact sensitive info even from error contexts.\n",
        "        context['user_email'] = '***redacted***'\n",
        "    logger.error(f\"Application error: {error_message}. Context: {context}\")\n",
        "\n",
        "\n",
        "# --- Example Usage ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"--- Demonstrating Logging for a Web Application ---\\n\")\n",
        "\n",
        "    # Simulate an incoming request with sensitive data\n",
        "    logger.info(\"Simulating an incoming request...\")\n",
        "    request_1 = {\n",
        "        'method': 'POST',\n",
        "        'path': '/login',\n",
        "        'ip_address': '192.168.1.100',\n",
        "        'username': 'testuser',\n",
        "        'password': 'supersecretpassword',\n",
        "        'email': 'user@example.com',\n",
        "        'auth_token': 'jwt.eyJhbGciOiJIUzI1NiJ9.eyJ1c2VySWQiOiIxMjMifQ.signature'\n",
        "    }\n",
        "    handle_request(request_1)\n",
        "\n",
        "    # Simulate a successful action\n",
        "    logger.info(\"Simulating a successful action...\")\n",
        "    process_data(user_id=\"user_abc_123\", data=\"User settings updated for theme_dark\")\n",
        "\n",
        "    # Simulate an error with some context\n",
        "    logger.info(\"Simulating an application error...\")\n",
        "    simulate_error(\"Database connection failed\", user_context={'user_id': 'user_abc_123', 'session_id': 'xyz123', 'user_email': 'user@example.com'})\n",
        "\n",
        "    print(\"\\n--- Check 'app.log' file for log output. --- \")\n",
        "    # Clean up the log file if it exists for a fresh run next time\n",
        "    if os.path.exists('app.log'):\n",
        "        # Developer Responsibility: Log files can grow large and contain historical data.\n",
        "        # Implement log rotation and secure archiving/deletion policies compliant with data retention laws.\n",
        "        # This simple example just removes it for repeated runs.\n",
        "        # os.remove('app.log') # Uncomment to remove log file after each run\n",
        "        pass\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "__main__ - INFO - Logging configured.\n",
            "INFO:__main__:Logging configured.\n",
            "__main__ - INFO - Simulating an incoming request...\n",
            "INFO:__main__:Simulating an incoming request...\n",
            "__main__ - INFO - Request received: {'method': 'POST', 'path': '/login', 'ip_address': '192.168.1.100', 'username': 'testuser', 'email': 'u***@example.com'}\n",
            "INFO:__main__:Request received: {'method': 'POST', 'path': '/login', 'ip_address': '192.168.1.100', 'username': 'testuser', 'email': 'u***@example.com'}\n",
            "__main__ - INFO - Simulating a successful action...\n",
            "INFO:__main__:Simulating a successful action...\n",
            "__main__ - INFO - Data successfully processed for user_id: user_abc_123. Data summary: User settings update...\n",
            "INFO:__main__:Data successfully processed for user_id: user_abc_123. Data summary: User settings update...\n",
            "__main__ - INFO - Simulating an application error...\n",
            "INFO:__main__:Simulating an application error...\n",
            "__main__ - ERROR - Application error: Database connection failed. Context: {'user_id': 'user_abc_123', 'session_id': 'xyz123', 'user_email': '***redacted***'}\n",
            "ERROR:__main__:Application error: Database connection failed. Context: {'user_id': 'user_abc_123', 'session_id': 'xyz123', 'user_email': '***redacted***'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Demonstrating Logging for a Web Application ---\n",
            "\n",
            "\n",
            "--- Check 'app.log' file for log output. --- \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b459f6a",
        "outputId": "d756e133-0b8d-45ac-a6e0-d2ec2a5a31f5"
      },
      "source": [
        "# Python program for simple product recommendation with ethical and fairness considerations\n",
        "\n",
        "from collections import Counter # Suggestion: Use Counter to easily count frequency of items\n",
        "\n",
        "def recommend_products(user_history: list[str]) -> list[str]:\n",
        "    \"\"\"Recommends products based on a simple user history (e.g., categories).\n",
        "\n",
        "    This function uses a frequency-based approach and includes inline comments to highlight\n",
        "    ethical considerations, potential biases, and developer responsibilities in AI-assisted\n",
        "    recommendation systems.\n",
        "\n",
        "    Args:\n",
        "        user_history (list[str]): A list of product categories previously viewed or purchased by the user.\n",
        "\n",
        "    Returns:\n",
        "        list[str]: A list of recommended product categories.\n",
        "    \"\"\"\n",
        "\n",
        "    # Predefined product catalog (simplified for demonstration)\n",
        "    # Developer Responsibility: Real-world catalogs are much larger and more complex. Ensure diversity and avoid biases in catalog creation.\n",
        "    product_catalog = {\n",
        "        \"Electronics\": [\"Smartphone\", \"Laptop\", \"Headphones\", \"Smartwatch\"],\n",
        "        \"Books\": [\"Fiction Novel\", \"Self-Help Book\", \"Biography\", \"Science Fiction\"],\n",
        "        \"Clothing\": [\"T-Shirt\", \"Jeans\", \"Jacket\", \"Sneakers\"],\n",
        "        \"Home & Kitchen\": [\"Coffee Maker\", \"Blender\", \"Cookware Set\", \"Smart Speaker\"],\n",
        "        \"Outdoor\": [\"Tent\", \"Hiking Boots\", \"Backpack\", \"Camping Chair\"],\n",
        "        \"Gaming\": [\"Gaming Console\", \"Video Game\", \"Gaming Headset\", \"Gaming Mouse\"]\n",
        "    }\n",
        "\n",
        "    print(f\"\\n--- User History: {user_history} ---\")\n",
        "\n",
        "    # Recommendation Logic: Frequency-based\n",
        "    # Suggestion: Count the frequency of each category in the user's history.\n",
        "    # Transparency/Explainability: This logic is simple: users like what they've interacted with most.\n",
        "    category_counts = Counter(user_history)\n",
        "    print(f\"Category frequency in history: {category_counts}\")\n",
        "\n",
        "    if not category_counts:\n",
        "        # Ethical Consideration: For new users or users with sparse history, provide diverse general recommendations.\n",
        "        # Avoid immediately pushing popular items which can create filter bubbles for new users.\n",
        "        return product_catalog.get(\"Electronics\", []) + product_catalog.get(\"Books\", []) # Simple default for no history\n",
        "\n",
        "    # Get the most frequently interacted-with category\n",
        "    most_common_category, _ = category_counts.most_common(1)[0]\n",
        "    print(f\"Most common category: {most_common_category}\")\n",
        "\n",
        "    # Recommend products from the most common category\n",
        "    # Transparency/Explainability: We are recommending items directly related to their past strong interest.\n",
        "    recommendations = product_catalog.get(most_common_category, [])\n",
        "\n",
        "    # Potential Fairness Issue: Popularity Bias\n",
        "    # Recommending only from the most popular items within a category can lead to popularity bias.\n",
        "    # Items that are already popular get more visibility, making them even more popular, while niche or new items are overlooked.\n",
        "    # Bias Effect: Reduces diversity, creates 'rich-get-richer' dynamics, and can prevent users from discovering less-known but relevant products.\n",
        "\n",
        "    # Potential Fairness Issue: Filter Bubbles / Echo Chambers\n",
        "    # Only recommending based on past similar items can trap users in a 'filter bubble'.\n",
        "    # They are only exposed to information/products that align with their existing preferences, limiting their perspective.\n",
        "    # Bias Effect: Decreases serendipity, limits user exploration, and can reinforce existing biases.\n",
        "\n",
        "    # Potential Fairness Issue: Exclusion of New or Niche Products\n",
        "    # Simple frequency-based systems struggle to recommend new products (cold start problem) or items from niche categories.\n",
        "    # Bias Effect: Disadvantages smaller businesses, independent creators, or less mainstream interests.\n",
        "\n",
        "    # Strategy to Promote Ethical Recommendations: Diversify Suggestions\n",
        "    # Suggestion: Introduce recommendations from related or complementary categories, or even some randomly selected diverse items.\n",
        "    # This helps break filter bubbles and expose users to new possibilities.\n",
        "    # Ethical Practice: Ensure a balance between relevance and discovery.\n",
        "    if most_common_category == \"Books\":\n",
        "        recommendations.extend(product_catalog.get(\"Home & Kitchen\", [])[:1]) # Suggest a complementary category item\n",
        "    elif most_common_category == \"Electronics\":\n",
        "        recommendations.extend(product_catalog.get(\"Gaming\", [])[:1])\n",
        "    # Developer Responsibility: Implement mechanisms for exploration (e.g., 'customers also bought', 'trending now', 'new arrivals').\n",
        "\n",
        "    # Strategy to Promote Ethical Recommendations: Avoid Discrimination\n",
        "    # While this simple model doesn't explicitly use demographic data, more complex systems might.\n",
        "    # Ethical Practice: Ensure recommendation algorithms do not discriminate based on protected characteristics (gender, race, age, etc.).\n",
        "    # Bias Effect: If demographic data is implicitly or explicitly used, recommendations could be unfairly withheld or skewed for certain groups.\n",
        "    # Developer Responsibility: Audit algorithms for disparate impact and ensure fairness metrics are monitored.\n",
        "\n",
        "    # Strategy to Promote Ethical Recommendations: Transparency and Control\n",
        "    # Ethical Practice: Users should understand why they are seeing certain recommendations and have control over their preferences.\n",
        "    # For instance, allowing users to 'dislike' recommendations or modify their interests.\n",
        "    # Developer Responsibility: Provide clear explanations of how recommendations are generated and offer user-facing controls.\n",
        "\n",
        "    # Strategy to Promote Ethical Recommendations: Not Exploiting User Behavior\n",
        "    # Ethical Practice: Avoid recommending products that exploit user vulnerabilities (e.g., addictive behaviors, financial distress).\n",
        "    # Developer Responsibility: Design systems with user well-being in mind, not just engagement or profit maximization.\n",
        "\n",
        "    # Final Recommendations (simplified, showing a mix)\n",
        "    # Ethical Consideration: Prioritize diversity over pure frequency in some cases to broaden user experience.\n",
        "    final_recommendations = list(set(recommendations)) # Remove duplicates\n",
        "    final_recommendations = final_recommendations[:5] # Limit to a reasonable number\n",
        "\n",
        "    return final_recommendations\n",
        "\n",
        "\n",
        "# Main execution block for demonstration\n",
        "if __name__ == \"__main__\":\n",
        "    # Example User Histories\n",
        "    user_history_1 = [\"Books\", \"Books\", \"Fiction Novel\", \"Electronics\", \"Books\"]\n",
        "    user_history_2 = [\"Clothing\", \"Clothing\", \"Jeans\", \"Outdoor\", \"Clothing\"]\n",
        "    user_history_3 = [\"Gaming\", \"Gaming Console\"]\n",
        "    user_history_4 = [] # New user or no history\n",
        "\n",
        "    print(\"\\n--- Recommendation for User History 1 ---\")\n",
        "    recs1 = recommend_products(user_history_1)\n",
        "    print(f\"Recommended Products: {recs1}\")\n",
        "\n",
        "    print(\"\\n--- Recommendation for User History 2 ---\")\n",
        "    recs2 = recommend_products(user_history_2)\n",
        "    print(f\"Recommended Products: {recs2}\")\n",
        "\n",
        "    print(\"\\n--- Recommendation for User History 3 ---\")\n",
        "    recs3 = recommend_products(user_history_3)\n",
        "    print(f\"Recommended Products: {recs3}\")\n",
        "\n",
        "    print(\"\\n--- Recommendation for User History 4 (New User) ---\")\n",
        "    recs4 = recommend_products(user_history_4)\n",
        "    print(f\"Recommended Products: {recs4}\")\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Recommendation for User History 1 ---\n",
            "\n",
            "--- User History: ['Books', 'Books', 'Fiction Novel', 'Electronics', 'Books'] ---\n",
            "Category frequency in history: Counter({'Books': 3, 'Fiction Novel': 1, 'Electronics': 1})\n",
            "Most common category: Books\n",
            "Recommended Products: ['Biography', 'Self-Help Book', 'Coffee Maker', 'Science Fiction', 'Fiction Novel']\n",
            "\n",
            "--- Recommendation for User History 2 ---\n",
            "\n",
            "--- User History: ['Clothing', 'Clothing', 'Jeans', 'Outdoor', 'Clothing'] ---\n",
            "Category frequency in history: Counter({'Clothing': 3, 'Jeans': 1, 'Outdoor': 1})\n",
            "Most common category: Clothing\n",
            "Recommended Products: ['Jeans', 'T-Shirt', 'Jacket', 'Sneakers']\n",
            "\n",
            "--- Recommendation for User History 3 ---\n",
            "\n",
            "--- User History: ['Gaming', 'Gaming Console'] ---\n",
            "Category frequency in history: Counter({'Gaming': 1, 'Gaming Console': 1})\n",
            "Most common category: Gaming\n",
            "Recommended Products: ['Gaming Console', 'Gaming Mouse', 'Gaming Headset', 'Video Game']\n",
            "\n",
            "--- Recommendation for User History 4 (New User) ---\n",
            "\n",
            "--- User History: [] ---\n",
            "Category frequency in history: Counter()\n",
            "Recommended Products: ['Smartphone', 'Laptop', 'Headphones', 'Smartwatch', 'Fiction Novel', 'Self-Help Book', 'Biography', 'Science Fiction']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42852453",
        "outputId": "b7519afe-76b7-4218-c6c9-581d21ce1bd4"
      },
      "source": [
        "# Python function for basic sentiment analysis with ethical and bias considerations\n",
        "\n",
        "def analyze_sentiment(text: str) -> str:\n",
        "    \"\"\"Performs basic sentiment analysis on a given text string (Positive, Negative, or Neutral).\n",
        "\n",
        "    This function uses a simple keyword-based approach and includes inline comments to highlight\n",
        "    ethical considerations, potential biases, and developer responsibilities in AI-assisted\n",
        "    sentiment analysis.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text string to analyze.\n",
        "\n",
        "    Returns:\n",
        "        str: The sentiment label ('Positive', 'Negative', 'Neutral').\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert text to lowercase for case-insensitive matching\n",
        "    processed_text = text.lower()\n",
        "\n",
        "    # Suggestion: Define lists of positive, negative, and neutral keywords.\n",
        "    # Ethical Consideration: Keyword lists themselves can be a source of bias.\n",
        "    # For example, terms that are positive in one cultural context might be neutral or even negative in another.\n",
        "    # Developer Responsibility: Regularly review and update keyword lists, considering cultural nuances and diverse linguistic use.\n",
        "    positive_keywords = ['good', 'great', 'excellent', 'happy', 'love', 'amazing', 'fantastic', 'wonderful', 'joy', 'benefit', 'succeed']\n",
        "    negative_keywords = ['bad', 'terrible', 'horrible', 'sad', 'hate', 'awful', 'poor', 'fail', 'problem', 'difficult', 'crisis']\n",
        "\n",
        "    # Bias Source: Language Bias (English-centric)\n",
        "    # This simple approach is inherently biased towards English. Languages have different structures, idioms, and sentiment expressions.\n",
        "    # A word-based model trained only on English will perform poorly or incorrectly for other languages.\n",
        "    # Bias Effect: Leads to unfair or inaccurate sentiment classifications for non-English content, potentially marginalizing non-English speakers.\n",
        "\n",
        "    # Bias Source: Cultural Bias\n",
        "    # The perceived sentiment of a word or phrase can vary significantly across cultures.\n",
        "    # E.g., 'challenging' might be negative in some contexts, but positive ('opportunity') in others.\n",
        "    # Bias Effect: Sentiment classifications may reflect dominant cultural norms, leading to misinterpretations for texts from minority cultures.\n",
        "\n",
        "    # Initialize sentiment scores\n",
        "    positive_score = 0\n",
        "    negative_score = 0\n",
        "\n",
        "    # Check for positive keywords\n",
        "    for keyword in positive_keywords:\n",
        "        if keyword in processed_text:\n",
        "            positive_score += 1\n",
        "\n",
        "    # Check for negative keywords\n",
        "    for keyword in negative_keywords:\n",
        "        if keyword in processed_text:\n",
        "            negative_score += 1\n",
        "\n",
        "    # Bias Source: Sarcasm and Irony\n",
        "    # Rule-based sentiment analysis struggles with sarcasm (\"Oh, that's just *great*.\") or irony, where words convey opposite meanings.\n",
        "    # Bias Effect: Misclassifications due to literal interpretation can lead to inaccurate insights, impacting decisions based on sentiment.\n",
        "\n",
        "    # Bias Source: Over-representation of Certain Viewpoints/Topics in Training Data (if using ML)\n",
        "    # While this is a rule-based model, if keywords were derived from biased datasets, it carries that bias.\n",
        "    # If an ML model were used, training data might disproportionately feature certain political, social, or demographic viewpoints.\n",
        "    # Bias Effect: The model's sentiment prediction will be skewed towards the dominant viewpoints, suppressing or misinterpreting others.\n",
        "    # This can lead to algorithmic unfairness and echo chambers.\n",
        "\n",
        "    # Determine sentiment based on scores\n",
        "    if positive_score > negative_score:\n",
        "        sentiment = 'Positive'\n",
        "    elif negative_score > positive_score:\n",
        "        sentiment = 'Negative'\n",
        "    else:\n",
        "        # Suggestion: Acknowledge neutrality or ambiguity when scores are equal or low.\n",
        "        sentiment = 'Neutral'\n",
        "\n",
        "    # Strategy to Handle/Reduce Bias: Balanced Datasets\n",
        "    # For ML models, ensure training data is representative across different demographics, languages, cultures, and viewpoints.\n",
        "    # For rule-based models, ensure keyword lists are compiled from diverse sources and regularly reviewed by diverse teams.\n",
        "\n",
        "    # Strategy to Handle/Reduce Bias: Neutral Wording and Contextual Analysis\n",
        "    # Encourage users to provide context where possible. For automated systems, explore more advanced NLP techniques\n",
        "    # that can understand context, negation, and discourse structures rather than just keywords.\n",
        "\n",
        "    # Strategy to Handle/Reduce Bias: Human Review and Intervention\n",
        "    # Implement human-in-the-loop systems, especially for high-stakes decisions. Humans can identify and correct biased outputs.\n",
        "    # Regular audits of sentiment analysis results by diverse groups can reveal patterns of bias.\n",
        "\n",
        "    # Strategy to Handle/Reduce Bias: Transparency and Explainability\n",
        "    # Clearly communicate the limitations and potential biases of the sentiment analysis model to users.\n",
        "    # Provide explanations for sentiment predictions (e.g., highlighting keywords) to allow for user understanding and correction.\n",
        "\n",
        "    # Developer Responsibility: Continuous Monitoring and Ethical Sourcing\n",
        "    # Developers are responsible for continuously monitoring model performance for disparate impact across groups.\n",
        "    # They must ensure data for model development (and keyword lists) is ethically sourced, respecting privacy and consent.\n",
        "    # Ethical AI: Prioritize fairness, accountability, and transparency in all stages of AI development and deployment.\n",
        "\n",
        "    return sentiment\n",
        "\n",
        "# Example Usage:\n",
        "if __name__ == \"__main__\":\n",
        "    text1 = \"This movie was absolutely amazing and I loved every moment of it!\"\n",
        "    text2 = \"The service was terrible and I had a horrible experience.\"\n",
        "    text3 = \"The weather today is neither good nor bad.\"\n",
        "    text4 = \"Oh, that's just *great* – another system crash. (Demonstrates sarcasm challenge)\"\n",
        "\n",
        "    print(f\"Text: '{text1}' -> Sentiment: {analyze_sentiment(text1)}\")\n",
        "    print(f\"Text: '{text2}' -> Sentiment: {analyze_sentiment(text2)}\")\n",
        "    print(f\"Text: '{text3}' -> Sentiment: {analyze_sentiment(text3)}\")\n",
        "    print(f\"Text: '{text4}' -> Sentiment: {analyze_sentiment(text4)}\") # Will incorrectly be 'Positive' due to 'great'\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: 'This movie was absolutely amazing and I loved every moment of it!' -> Sentiment: Positive\n",
            "Text: 'The service was terrible and I had a horrible experience.' -> Sentiment: Negative\n",
            "Text: 'The weather today is neither good nor bad.' -> Sentiment: Neutral\n",
            "Text: 'Oh, that's just *great* – another system crash. (Demonstrates sarcasm challenge)' -> Sentiment: Positive\n"
          ]
        }
      ]
    }
  ]
}